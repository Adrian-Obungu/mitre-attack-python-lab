# Performance and Load Testing Suite

This suite contains scripts and tools to help you benchmark, profile, and load test the applications in this project.

**IMPORTANT**: Running these tests, especially the load tests, will generate a significant amount of network traffic and consume system resources. Run them in a controlled environment and ensure you have permission to test against any target that is not `localhost`.

## 1. Setup

Before running these tests, you will need to install some additional dependencies:

```bash
pip install memory-profiler matplotlib
```

## 2. Test Scripts

This suite includes the following scripts:

*   `test_port_scan_load.py`: Simulates a high volume of concurrent port scans to stress-test the scanning tools.
*   `test_dns_honeypot_load.py`: Generates a high volume of DNS queries to load test the `HoneyResolver_Enhanced.py` script.
*   `profile_memory.py`: Runs the DNS honeypot for a specified duration while tracking its memory usage to identify potential memory leaks.
*   `profile_cpu.py`: Uses `cProfile` to profile the DNS honeypot under load, helping to identify performance bottlenecks in the code.
*   `plot_results.py`: A utility to generate a graph from the output of the memory profiling script.

## 3. How to Run the Tests

### 3.1. DNS Honeypot Load and Profile Test

This test requires two separate terminals.

**Terminal 1: Run the Honeypot (with or without profiling)**

First, start the DNS honeypot. 

*   **For the load test or memory profiling**, run the honeypot normally:
    ```bash
    python src/defense/HoneyResolver_Enhanced.py
    ```
*   **For CPU profiling**, run the honeypot via the `profile_cpu.py` script. This will start the server and automatically profile it when it's under load.
    ```bash
    python performance_testing/profile_cpu.py
    ```

**Terminal 2: Run the DNS Load Generator**

Once the honeypot is running, open a second terminal and run the DNS load testing script. This will send a high volume of queries to the honeypot.

```bash
# Example: 500 QPS for 30 seconds
python performance_testing/test_dns_honeypot_load.py --qps 500 --duration 30
```

### 3.2. Memory Profiling (Soak Test)

To test for memory leaks over a long period, run the `profile_memory.py` script. This will start the honeypot and log its memory usage to a file.

```bash
# Example: Run for 60 minutes, sampling memory every 10 seconds
python performance_testing/profile_memory.py --duration 3600 --interval 10
```

After the test is complete, you can generate a graph of the memory usage:

```bash
python performance_testing/plot_results.py memory_usage.dat
```

This will create a `memory_usage.png` graph.

### 3.3. Port Scan Load Test

This script will simulate multiple concurrent scans.

```bash
# Example: Simulate 50 concurrent scans, each scanning 100 ports on localhost
python performance_testing/test_port_scan_load.py --workers 50 --ports 1-100 --target localhost
```

## 4. Interpreting the Results

*   **CPU Profile (`profile_output.pstats`)**:
    *   This file is generated by `profile_cpu.py`. You can analyze it using Python's `pstats` module.
    *   Start a Python interpreter and run:
        ```python
        import pstats
        p = pstats.Stats('profile_output.pstats')
        p.sort_stats('cumulative').print_stats(20)
        ```
    *   Look for functions with a high `cumtime` (cumulative time). These are the functions where your program spends most of its time and are the best candidates for optimization.

*   **Memory Usage Graph (`memory_usage.png`)**:
    *   This graph shows memory usage over time.
    *   A healthy application should have a relatively flat memory usage graph over a long period.
    *   If the graph consistently trends upwards without plateauing, it may indicate a memory leak.

*   **Load Test Output**:
    *   The load testing scripts will report metrics like Queries Per Second (QPS) or Scans Per Second.
    *   You can use these numbers to establish a performance baseline. If you make changes to the code, run the tests again and compare the new results to the baseline to see if performance has improved or regressed.

## 5. Actionable Recommendations for Optimization

Based on a static analysis of the code and common performance patterns in network applications, here are several recommendations for optimization. Use the profiling tools to confirm which of these are most relevant to your application.

### 5.1. DNS Honeypot (`HoneyResolver_Enhanced.py`)

1.  **Asynchronous Architecture**:
    *   **Observation**: The current honeypot uses `dnslib`'s standard `DNSServer`, which is synchronous and handles requests one by one in a single thread. This will be the primary bottleneck under high load.
    *   **Recommendation**: For significantly higher QPS, consider migrating to an asynchronous DNS server implementation. Libraries like `asyncio` combined with a suitable DNS library (or even a custom, lightweight async implementation) would allow the server to handle many concurrent requests without being blocked by network I/O.

2.  **Logging Performance**:
    *   **Observation**: The script logs every single query to both the console (`StreamHandler`) and a file (`FileHandler`). Disk I/O is slow and can become a bottleneck.
    *   **Recommendation**: For high-performance scenarios, consider disabling the `StreamHandler` or reducing the logging level for console output. You could also make logging fully asynchronous by sending log messages to a separate process or thread to be written to disk, decoupling the main application logic from logging I/O.

3.  **Configuration Loading**:
    *   **Observation**: The configuration is currently loaded once at startup, which is good. 
    *   **Recommendation**: This is already well-optimized. No changes are recommended here unless you need dynamic, on-the-fly configuration reloading.

### 5.2. Log Parser (`log_parser.py`)

1.  **Regex Performance**:
    *   **Observation**: The regex for parsing logs was identified as a potential ReDoS vector and has been improved. However, regex is still computationally expensive, especially when parsing millions of log lines.
    *   **Recommendation**: While the current regex is much safer, if profiling shows `_parse_log_line` is still a bottleneck, consider moving away from regex altogether. A more performant approach would be to log in a structured format (like JSON or key-value pairs) from the honeypot. This would make parsing in `log_parser.py` a simple `json.loads()` or string split operation, which is significantly faster than regex matching.

### 5.3. General Python Performance

*   **Use Caching**: If you find that certain operations are frequently repeated with the same inputs (e.g., looking up the same subdomain), consider using a caching mechanism like `functools.lru_cache` to memoize the results.
*   **Avoid Global Variables**: Accessing global variables can be slightly slower than local variables. Passing configuration and state explicitly through class instances (as has been done in the refactored code) is generally better for performance and maintainability.
*   **Choose Appropriate Data Structures**: The current use of `defaultdict` and `Counter` in `log_parser.py` is efficient for the task. Continue to use appropriate, high-performance data structures for data aggregation and analysis.